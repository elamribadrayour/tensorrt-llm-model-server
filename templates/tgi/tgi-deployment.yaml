{{- if .Values.tgi.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi
  namespace: tensorrt-llm-model-server
spec:
  selector:
    matchLabels:
      app: tgi
  template:
    metadata:
      labels:
        app: tgi
    spec:
      containers:
        - name: tgi-container
          imagePullPolicy: Always
          image: {{ .Values.tgi.image }}
          command: ["text-generation-launcher"]
          args: [
            "--model-id", "{{ .Values.tgi.model }}",
            "--lora-adapters", "{{ .Values.tgi.lora_adapters }}",
            "--port", "8080",
            "--max-input-tokens", "{{ .Values.tgi.maxInputTokens }}",
            "--max-total-tokens", "{{ .Values.tgi.maxTotalTokens }}",
            "--max-batch-prefill-tokens", "{{ .Values.tgi.maxBatchPrefillTokens }}",
            "--max-batch-size", "{{ .Values.tgi.maxBatchSize }}",
            "--max-concurrent-requests", "{{ .Values.tgi.maxConcurrentRequests }}"
          ]
          ports:
            - containerPort: 8080
              name: http
          volumeMounts:
            - name: model-cache
              mountPath: /data
            - name: huggingface-token
              mountPath: /root/.cache/huggingface
              readOnly: true
          readinessProbe:
            httpGet:
              path: /info
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /info
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            # GPU Types and their characteristics:
            # - nvidia-l4: 24GB VRAM, Ada Lovelace architecture, supports FlashAttention (current config)
            # - nvidia-tesla-t4: 16GB VRAM, Turing architecture, no FlashAttention
            # - nvidia-tesla-a100: 40GB VRAM, Ampere architecture, supports FlashAttention
            # - nvidia-a100-80gb: 80GB VRAM, Ampere architecture, supports FlashAttention
            # - nvidia-h100-80gb: 80GB VRAM, Hopper architecture, supports FlashAttention
            # - nvidia-h100-mega-80gb: 80GB VRAM, Hopper architecture, supports FlashAttention
            # - nvidia-h200-141gb: 141GB VRAM, Hopper architecture, supports FlashAttention
            # - nvidia-b200: 180GB VRAM, Blackwell architecture, supports FlashAttention
            #
            # Note: CPU and RAM requirements are minimal as the model runs on GPU:
            # - CPU: 1-2 cores for request handling and system operations
            # - RAM: 4-8GB for system operations and request buffering
            requests:
              cpu: {{ .Values.tgi.requests.cpu }}
              memory: {{ .Values.tgi.requests.memory }}
              ephemeral-storage: 10Gi
              nvidia.com/gpu: "1"  # Request 1 GPU
            limits:
              cpu: {{ .Values.tgi.limits.cpu }}
              memory: {{ .Values.tgi.limits.memory }}
              nvidia.com/gpu: "1"  # Limit to 1 GPU
              ephemeral-storage: 20Gi
          env:
            - name: HUGGINGFACE_HUB_CACHE
              value: "/data/hub"
            - name: LANGFUSE_HOST
              value: http://langfuse-web.langfuse.svc.cluster.local:3000
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-token
                  key: token
      # GPU Types and their use cases:
      # - nvidia-l4: General-purpose GPU, good for ML inference and video processing
      # - nvidia-tesla-t4: Cost-effective for ML inference and general GPU workloads
      # - nvidia-tesla-a100: A100 40GB, excellent for large ML models and HPC
      # - nvidia-a100-80gb: A100 80GB, ideal for very large ML models and memory-intensive workloads
      # - nvidia-h100-80gb: H100 80GB, best for cutting-edge ML training and inference
      # - nvidia-h100-mega-80gb: H100 Mega 80GB, specialized for large-scale ML workloads
      # - nvidia-h200-141gb: H200 141GB, optimized for large language models and AI workloads
      # - nvidia-b200: B200 180GB, designed for the most demanding AI and HPC workloads
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: huggingface-token
          secret:
            secretName: huggingface-token
            items:
              - key: token
                path: token
      nodeSelector:
        cloud.google.com/gke-spot: "true"
        cloud.google.com/gke-accelerator: nvidia-l4
      terminationGracePeriodSeconds: 25
{{- end }}